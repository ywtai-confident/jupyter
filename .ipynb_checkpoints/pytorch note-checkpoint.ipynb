{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887f613b",
   "metadata": {},
   "source": [
    "# 基础数据操作相关API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ad47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5b2714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.float32\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b05272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7.5554e+28, 1.7777e+31, 5.2392e-11],\n",
      "        [5.3379e+31, 4.2964e+24, 7.5555e+31]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9f6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f65fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.eye(2, 3) # 一般用于生成单位矩阵\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4679813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7e0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones_like(a) # 与a形状相同的\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d62e7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3119, 0.2701],\n",
      "        [0.1118, 0.1012]])\n",
      "tensor([0.1790, 0.0072, 0.0977, 0.1582, 0.0467])\n",
      "tensor([0.3437, 0.4959, 0.9157, 1.3216, 0.5133])\n",
      "[[-4.84831321e-02  9.98318002e+01]\n",
      " [ 7.32938291e-02  1.00794624e+02]\n",
      " [-6.44526809e-01  9.88820567e+01]\n",
      " [-1.09776913e-01  9.82615132e+01]\n",
      " [ 1.37930820e-01  9.76756827e+01]\n",
      " [-1.82387258e+00  1.00031312e+02]\n",
      " [ 2.63948948e-01  1.00460321e+02]\n",
      " [-1.55794891e+00  1.01989798e+02]\n",
      " [ 4.91901035e-01  9.90212363e+01]\n",
      " [-1.36130821e+00  9.88244449e+01]]\n"
     ]
    }
   ],
   "source": [
    "# 随机分布\n",
    "# 随机种子\n",
    "torch.manual_seed(666)\n",
    "a = torch.rand(2, 2) # 从[0, 1)之间的均匀分布中采样\n",
    "print(a)\n",
    "a = torch.normal(mean=0, std=torch.rand(5)) # 5个分布中每个分布随机采1个值(均值相同)\n",
    "print(a)\n",
    "a = torch.normal(mean=torch.rand(5), std=torch.rand(5)) # 5个分布中每个分布随机采1个值\n",
    "print(a)\n",
    "a = np.random.normal(loc=[0, 100], scale=[1, 2], size=[10, 2]) \n",
    "# numpy也可以！同时从2个分布中采样，“一列”对应一个分布，torch还不支持这种操作\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629cfd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4072,  0.1162, -0.8812,  0.9023,  0.1591],\n",
       "        [-0.8144, -0.9872, -0.1468, -0.1269, -0.0321],\n",
       "        [ 0.6604,  0.6064, -0.9005, -0.0965,  0.2162],\n",
       "        [ 0.7332, -0.3575, -0.7809, -0.7300,  0.0055],\n",
       "        [ 0.1569,  0.8255,  0.7798, -0.2301, -0.4436]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(5, 5).uniform_(-1, 1) # 均匀分布\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98225024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5238ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 2.,  6., 10.])\n"
     ]
    }
   ],
   "source": [
    "# 序列\n",
    "a = torch.arange(1, 10, 1)\n",
    "print(a)\n",
    "a = torch.linspace(2, 10, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0689356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 2, 1, 3, 8, 9, 4, 0, 6, 7])\n",
      "[0 6 7 3 9 5 1 8 4 2]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(10) # 打乱索引\n",
    "print(a)\n",
    "a = np.random.permutation(10)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd25f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4b6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([[0, 0, 3, 0],\n",
      "        [4, 0, 5, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# tensor的属性\n",
    "# torch.device('cuda:0') mac m1没有cuda...\n",
    "a = torch.tensor([1,2], dtype=torch.float32, device=torch.device('cpu'))\n",
    "print(a)\n",
    "# 稀疏张量\n",
    "indices = torch.tensor([[0, 1, 1], [2, 0, 2]])\n",
    "values = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "x = torch.sparse_coo_tensor(indices, values, [2, 4], dtype=torch.int32, device=torch.device('cpu'))\n",
    "print(x.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eab8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[  90.0171, 2980.9580]])\n",
      "tensor([[  90.0171, 2980.9580]])\n",
      "tensor([0.0000, 1.0000, 0.6021])\n",
      "tensor([0.0000, 1.0000, 0.6021])\n",
      "tensor([0.0000, 2.3026, 1.3863])\n"
     ]
    }
   ],
   "source": [
    "# 代数运算\n",
    "# 加减: add, sub\n",
    "a = torch.tensor([2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = a + b\n",
    "print(c)\n",
    "c = torch.add(a, b)\n",
    "print(c)\n",
    "c = a.add(b)\n",
    "print(c)\n",
    "a.add_(b) # a = a + b 覆盖a的值，其他运算下划线含义一致\n",
    "print(a)\n",
    "\n",
    "# 乘 mul，为哈达玛积（element wise，对应元素相乘），除法div含义类似\n",
    "c = a * b\n",
    "print(c)\n",
    "c = torch.mul(a, b)\n",
    "print(c)\n",
    "c = a.mul(b)\n",
    "print(c)\n",
    "a.mul_(b)\n",
    "print(a)\n",
    "\n",
    "# 矩阵运算\n",
    "a = torch.Tensor([[1], [4]])\n",
    "b = torch.ones(1, 2) * 10\n",
    "print(torch.matmul(a, b))\n",
    "print(a @ b)\n",
    "print(a.matmul(b))\n",
    "\n",
    "# 高维度的矩阵运算同样支持，但矩阵乘法仅定义在最后2个维度上，且要求前面的维度保持一致\n",
    "a = torch.ones(1, 2, 3, 4)\n",
    "b = torch.ones(1, 2, 4, 2)\n",
    "print((a @ b).shape)\n",
    "print(a.matmul(b).shape)\n",
    "print(torch.matmul(a, b).shape)\n",
    "\n",
    "# 幂运算\n",
    "a = torch.Tensor([[3, 4]])\n",
    "print(torch.pow(a, 2))\n",
    "print(a.pow(2))\n",
    "print(a**2)\n",
    "a.pow_(2)\n",
    "print(a)\n",
    "print(torch.exp(a)) # e^a\n",
    "print(a.exp())\n",
    "a.exp_()\n",
    "print(a)\n",
    "\n",
    "# 开根号\n",
    "print(a.sqrt())\n",
    "a.sqrt_()\n",
    "print(a)\n",
    "\n",
    "# 对数运算\n",
    "a = torch.tensor([1, 10, 4], dtype=torch.float32)\n",
    "print(a.log10())\n",
    "print(torch.log10(a))\n",
    "a.log_() # log默认以e为底\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d1a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.0602, 1.5524, 1.3197]],\n",
      "\n",
      "        [[0.7598, 1.4492, 0.9827]]])\n",
      "torch.Size([2, 1, 3])\n",
      "[[[[1.47118582 1.33780221 0.54004127]\n",
      "   [1.04589429 1.16738278 0.41124551]]\n",
      "\n",
      "  [[0.73005656 1.35408101 1.39176608]\n",
      "   [0.58443805 0.61709771 1.35733496]]\n",
      "\n",
      "  [[0.77535224 0.70587079 0.30410549]\n",
      "   [1.18699594 0.88603694 0.74607413]]\n",
      "\n",
      "  [[1.31243008 0.6470214  1.07103127]\n",
      "   [1.89980652 0.57026855 1.7894615 ]]]\n",
      "\n",
      "\n",
      " [[[0.83008059 1.20883149 0.85903534]\n",
      "   [0.40478905 1.03841206 0.73023958]]\n",
      "\n",
      "  [[1.44618569 1.09474359 1.42894961]\n",
      "   [1.30056718 0.3577603  1.39451849]]\n",
      "\n",
      "  [[1.28513222 0.70842977 1.22537784]\n",
      "   [1.69677592 0.88859592 1.66734649]]\n",
      "\n",
      "  [[1.1561804  0.74955137 0.18614949]\n",
      "   [1.74355684 0.67279852 0.90457972]]]]\n",
      "(2, 4, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in-place操作：就地操作，也称为原位操作，不允许使用临时变量，如加法add_等带下划线的操作\n",
    "广播机制：满足一定条件，参与运算的2个变量的shape可自动对齐\n",
    "1.每个张量至少有一个维度\n",
    "2.满足右对齐：如a->shape(2, 1, 3), b->shape(3,),a+b时,b会被扩展到(1, 1, 3),在逐个比较shape的每个维度满足：值相同或有一个为1\n",
    "\"\"\"\n",
    "c = torch.rand(2, 1, 3) + torch.rand(3,)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "# numpy同样满足广播机制\n",
    "c = np.random.rand(2, 4, 1, 3) + np.random.rand(4, 2, 3)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b698c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.4637, 4.0186],\n",
      "        [2.8766, 6.7941]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 6.]])\n",
      "tensor([[2., 5.],\n",
      "        [3., 7.]])\n",
      "tensor([[1., 4.],\n",
      "        [3., 7.]])\n",
      "tensor([[1., 4.],\n",
      "        [2., 6.]])\n",
      "tensor([[0.4637, 0.0186],\n",
      "        [0.8766, 0.7941]])\n",
      "tensor([[1.4637, 0.0186],\n",
      "        [0.8766, 0.7941]])\n"
     ]
    }
   ],
   "source": [
    "# 取整｜取余\n",
    "a = torch.rand(2, 2)\n",
    "a *= 10\n",
    "print(a)\n",
    "print(torch.floor(a)) # 向上取整\n",
    "print(torch.ceil(a)) # 向下取整\n",
    "print(torch.round(a)) # 四舍五入\n",
    "print(torch.trunc(a)) # 裁剪，只取整数部分\n",
    "print(torch.frac(a)) # 只取小数部分\n",
    "print(a % 2) # 取余"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "d4564e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False,  True, False]])\n",
      "tensor([[False, False, False],\n",
      "        [False,  True, False]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# tensor比较运算\n",
    "a = torch.tensor([[2, 1, 3], [5, 1, 2]])\n",
    "b = torch.tensor([[2, 1, 3], [5, 2, 2]])\n",
    "print(a == b)\n",
    "print(torch.eq(a, b)) # 逐个比较a和b中的元素，相等返回true，否则返回false，返回值也是一个tensor\n",
    "print(torch.ge(a, b)) # a >= b\n",
    "print(torch.gt(a, b)) # a > b\n",
    "print(torch.le(a, b)) # a <= b\n",
    "print(torch.lt(a, b)) # a < b\n",
    "print(torch.ne(a, b)) # a != b\n",
    "\n",
    "a = torch.tensor([[2, 1, 3], [5, 1, 2]])\n",
    "b = torch.tensor([[2, 1, 3], [5, 1, 2], [4, 1, 4]])\n",
    "print(torch.equal(a, b)) # 返回值为true或false，shape和每个对应元素都相同才返回true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "81f0449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0436, 0.1971, 0.2023, 0.9025, 0.9869],\n",
      "        [0.3196, 0.4349, 0.4473, 0.5565, 0.5772],\n",
      "        [0.2355, 0.2576, 0.4258, 0.5158, 0.7528]])\n",
      "tensor([[1, 0, 2, 3, 4],\n",
      "        [2, 1, 3, 4, 0],\n",
      "        [4, 3, 2, 0, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 排序｜前k大｜前k小｜第k小\n",
    "a = torch.rand(3, 5)\n",
    "# sort的第二个参数dim，0是沿着第一个维度（每列有序），1沿着第二个维度（每行有序），\n",
    "# -1等同于倒数第一个维度，-2依次类推，默认值-1\n",
    "sorts, indices = torch.sort(a)\n",
    "print(sorts)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2993b9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[5, 4],\n",
       "        [6, 5]]),\n",
       "indices=tensor([[4, 1],\n",
       "        [1, 2]]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2, 4, 3, 1, 5], [2, 6, 5, 1, 4]])\n",
    "# 寻找前k大的元素以及对应的索引\n",
    "torch.topk(a, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de97f044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([3, 4]),\n",
       "indices=tensor([2, 4]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 寻找按某个维度第k小的元素\n",
    "torch.kthvalue(a, 3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "733eb209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8472, 0.2549, 0.7015],\n",
      "        [0.2809, 0.3289, 0.2951]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "# 判断nan，inf，以及是否有界\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(a / 0)\n",
    "print(torch.isinf(a / 0)) # 判断是否为inf\n",
    "print(torch.isfinite(a / 0)) # 判断是否有界\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isnan(a)) # 判断是否为nan\n",
    "a = torch.tensor([1, np.nan, 2])\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "411d47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8415, -0.9093,  0.0000])\n",
      "tensor([ 0.5403, -0.4161,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch中的三角函数\n",
    "a = torch.tensor([1, -2, 0])\n",
    "print(torch.sin(a))\n",
    "print(torch.cos(a))\n",
    "# ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "74bf3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 0])\n",
      "tensor([ 1.0000, -0.5000,     inf])\n",
      "tensor([0.7311, 0.1192, 0.5000])\n",
      "tensor([1, 0, 0])\n",
      "tensor([ 1, -1,  0])\n"
     ]
    }
   ],
   "source": [
    "# torch中其他数学函数\n",
    "print(torch.abs(a))\n",
    "print(torch.reciprocal(a)) # 求倒数\n",
    "print(torch.sigmoid(a))\n",
    "print(torch.relu(a))\n",
    "print(torch.sign(a)) # 符号函数\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "010bbae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8333)\n",
      "torch.return_types.mode(\n",
      "values=tensor([-2.,  4.]),\n",
      "indices=tensor([1, 0]))\n",
      "tensor([1., 2., 0., 1., 2.])\n",
      "tensor([0, 6, 6, 8, 6, 5, 2, 2, 1, 1])\n",
      "tensor([1, 2, 2, 0, 0, 1, 3, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "# torch统计学函数：torch.mean, torch.max, torch.std......\n",
    "a = torch.tensor([[1, -2, 0], [4, 6, 8]], dtype=torch.float32)\n",
    "print(torch.mean(a))\n",
    "print(torch.mode(a))\n",
    "# ......\n",
    "\n",
    "# 计算直方图：参数：（input，bin，min，max）\n",
    "# 第二个参数bin表示等距划分为多少个区间（每个区间左闭右开，最后一个区间两边都是闭合的）\n",
    "# min和max表示统计的范围，当输入的min=max时，则取输入数据的最小值最大值替代；小于min和大于max的元素将被忽略\n",
    "print(torch.histc(a, 5, 0, 0))\n",
    "\n",
    "# 返回每个值的频数\n",
    "a = torch.randint(0, 10, (10,))\n",
    "print(a)\n",
    "# bincount只支持一维非负整数（比如处理聚类|分类结果）\n",
    "print(torch.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "ef530eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributions 定义分布函数\n",
    "# torch中还定义了衡量分布差异性的指标，如KL散度等\n",
    "# kl divergence|transforms|constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "75b4e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1962)\n",
      "tensor(2.4581)\n"
     ]
    }
   ],
   "source": [
    "# 范数\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5, 6])\n",
    "print(torch.dist(a, b, p=2))\n",
    "\n",
    "# 核范数（用于低秩约束）\n",
    "# torch.manual_seed(666)\n",
    "a = torch.rand(5, 2)\n",
    "print(torch.norm(a, p='nuc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "fb222d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-0.4143,  0.3777],\n",
       "        [-0.6097, -0.4521],\n",
       "        [-0.1738, -0.3588],\n",
       "        [-0.4661, -0.2697],\n",
       "        [-0.4573,  0.6719]]),\n",
       "S=tensor([1.1700, 0.3964]),\n",
       "V=tensor([[-0.6803, -0.7329],\n",
       "        [-0.7329,  0.6803]]))"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵分解（LU（上三角+下三角），QR（正交矩阵+上三角），EVD（PCA），SVD（LDA））\n",
    "# 详细内容见ipad笔记\n",
    "torch.svd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "0d29d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1190, 2.7013],\n",
      "        [1.1178, 1.0119]])\n",
      "tensor([[3.1190, 2.7013],\n",
      "        [2.0000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# tensor裁剪运算\n",
    "torch.manual_seed(666)\n",
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "print(a.clamp(2, 6)) # 数据保持在给定范围内，超过该范围替换为端点值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "18fbe581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8841, 0.6998, 0.2183, 0.8387],\n",
      "        [0.9829, 0.9044, 0.8159, 0.5987],\n",
      "        [0.0748, 0.8345, 0.4716, 0.1894],\n",
      "        [0.2303, 0.3802, 0.6539, 0.5621]])\n",
      "tensor([[0.5167, 0.1744, 0.7737, 0.6038],\n",
      "        [0.6025, 0.1154, 0.3679, 0.4395],\n",
      "        [0.9794, 0.8654, 0.3408, 0.7495],\n",
      "        [0.3279, 0.9853, 0.9525, 0.2259]])\n",
      "tensor([[0.8841, 0.6998, 0.7737, 0.8387],\n",
      "        [0.9829, 0.9044, 0.8159, 0.5987],\n",
      "        [0.9794, 0.8345, 0.3408, 0.7495],\n",
      "        [0.3279, 0.9853, 0.6539, 0.5621]])\n",
      "index_select------------------------------------------------------\n",
      "tensor([[0.2334, 0.8152, 0.9477, 0.7594],\n",
      "        [0.3135, 0.3180, 0.3245, 0.0351],\n",
      "        [0.1740, 0.9530, 0.3065, 0.8933],\n",
      "        [0.4779, 0.8937, 0.7975, 0.7827]])\n",
      "tensor([[0.2334, 0.8152, 0.9477, 0.7594],\n",
      "        [0.4779, 0.8937, 0.7975, 0.7827],\n",
      "        [0.1740, 0.9530, 0.3065, 0.8933]])\n",
      "tensor([[0.2334, 0.8152, 0.9477, 0.7594],\n",
      "        [0.4779, 0.8937, 0.7975, 0.7827],\n",
      "        [0.1740, 0.9530, 0.3065, 0.8933]])\n",
      "gather------------------------------------------------------\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[1, 6, 7, 8],\n",
      "        [9, 6, 3, 4]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "take------------------------------------------------------\n",
      "tensor([ 1, 16, 14, 11])\n",
      "nonzero------------------------------------------------------\n",
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 3]])\n"
     ]
    }
   ],
   "source": [
    "# tensor索引 数据筛选|重新组合\n",
    "a = torch.rand(4, 4)\n",
    "b = torch.rand(4, 4)\n",
    "print(a)\n",
    "print(b)\n",
    "# 每个位置根据条件判断，满足条件选择a的元素，否则选择b中的元素\n",
    "out = torch.where(a > 0.5, a, b)\n",
    "print(out)\n",
    "\n",
    "print('index_select------------------------------------------------------')\n",
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "# 指定维度上将不同索引的数据按照指定顺序重新组合\n",
    "print(torch.index_select(a, dim=0, index=torch.tensor([0, 3, 2])))\n",
    "print(a[[0, 3, 2]])\n",
    "\n",
    "# gather\n",
    "print('gather------------------------------------------------------')\n",
    "a = torch.arange(1, 17).view(4, 4)\n",
    "print(a)\n",
    "# 输出shape和index的shape相同（含义看打印结果吧），也是对原数据进行重组\n",
    "print(torch.gather(a, dim=0, index=torch.tensor([[0, 1, 1, 1], [2, 1, 0, 0]])))\n",
    "\n",
    "# select\n",
    "print(a)\n",
    "mask = torch.gt(a, 8)\n",
    "print(torch.masked_select(a, mask))\n",
    "print(torch.masked_select(a, a > 8))\n",
    "print(a[a > 8])\n",
    "\n",
    "print('take------------------------------------------------------')\n",
    "# take: 类似index_select，只是将a看成1维向量，再取对应的索引\n",
    "print(torch.take(a, index=torch.tensor([0, 15, 13, 10])))\n",
    "\n",
    "print('nonzero------------------------------------------------------')\n",
    "# nonzero: 返回非0元素的索引\n",
    "a = torch.tensor([[0, 1, 2, 0], [2, 3, 0, 1]])\n",
    "out = torch.nonzero(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "c88954a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor组合和拼接: torch.cat torch.stack\n",
    "a = torch.zeros(2, 4)\n",
    "b = torch.ones(2, 4)\n",
    "out = torch.cat([a, b], dim=0)\n",
    "print(out)\n",
    "\n",
    "# stack是拓展了新的维度再堆叠数据\n",
    "a = torch.arange(1, 7).view(2, 3)\n",
    "b = torch.arange(7, 13).view(2, 3)\n",
    "c = torch.arange(7, 13).view(2, 3)\n",
    "out = torch.stack([a, b, c], dim=0)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "1b6536d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.1516, 0.6787, 0.7833, 0.8108],\n",
      "        [0.2168, 0.5138, 0.7992, 0.7491],\n",
      "        [0.0815, 0.8425, 0.7910, 0.3669]]), tensor([[0.5206, 0.2379, 0.7905, 0.4837],\n",
      "        [0.6620, 0.6338, 0.4499, 0.7848]]))\n",
      "(tensor([[0.1516, 0.6787],\n",
      "        [0.2168, 0.5138],\n",
      "        [0.0815, 0.8425],\n",
      "        [0.5206, 0.2379],\n",
      "        [0.6620, 0.6338]]), tensor([[0.7833, 0.8108],\n",
      "        [0.7992, 0.7491],\n",
      "        [0.7910, 0.3669],\n",
      "        [0.7905, 0.4837],\n",
      "        [0.4499, 0.7848]]))\n",
      "(tensor([[0.1516, 0.6787, 0.7833, 0.8108],\n",
      "        [0.2168, 0.5138, 0.7992, 0.7491]]), tensor([[0.0815, 0.8425, 0.7910, 0.3669],\n",
      "        [0.5206, 0.2379, 0.7905, 0.4837]]), tensor([[0.6620, 0.6338, 0.4499, 0.7848]]))\n",
      "------------------------------------------------------\n",
      "(tensor([[0.1516, 0.6787, 0.7833, 0.8108]]), tensor([[0.2168, 0.5138, 0.7992, 0.7491],\n",
      "        [0.0815, 0.8425, 0.7910, 0.3669]]), tensor([[0.5206, 0.2379, 0.7905, 0.4837],\n",
      "        [0.6620, 0.6338, 0.4499, 0.7848]]))\n"
     ]
    }
   ],
   "source": [
    "# tensor切片: chunk split\n",
    "a = torch.rand(5, 4)\n",
    "# chunk：第2个参数表示切成多少份\n",
    "print(torch.chunk(a, 2, dim=0))\n",
    "print(torch.chunk(a, 2, dim=1))\n",
    "\n",
    "# split：第2个参数表示每份多少个，如输入3表示每份3个，输入[1, 2, ...]定义了每份多少个，不规则切片\n",
    "out = torch.split(a, 2, dim=0)\n",
    "print(out)\n",
    "print('------------------------------------------------------')\n",
    "out = torch.split(a, [1, 2, 2], dim=0)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "d83c2ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "------------------------------------------------------\n",
      "tensor([[ 0,  4,  8, 12],\n",
      "        [ 1,  5,  9, 13],\n",
      "        [ 2,  6, 10, 14],\n",
      "        [ 3,  7, 11, 15]])\n",
      "tensor([[ 0,  4,  8, 12],\n",
      "        [ 1,  5,  9, 13],\n",
      "        [ 2,  6, 10, 14],\n",
      "        [ 3,  7, 11, 15]])\n",
      "------------------------------------------------------\n",
      "torch.Size([3, 4])\n",
      "torch.Size([4, 3])\n",
      "------------------------------------------------------\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 1])\n",
      "------------------------------------------------------\n",
      "tensor([[0.1129, 0.9829, 0.1387, 0.9355],\n",
      "        [0.8607, 0.3921, 0.0525, 0.0413],\n",
      "        [0.9672, 0.5938, 0.1712, 0.8531]])\n",
      "(tensor([0.1129, 0.8607, 0.9672]), tensor([0.9829, 0.3921, 0.5938]), tensor([0.1387, 0.0525, 0.1712]), tensor([0.9355, 0.0413, 0.8531]))\n",
      "------------------------------------------------------\n",
      "tensor([[0.1129, 0.9829, 0.1387, 0.9355],\n",
      "        [0.8607, 0.3921, 0.0525, 0.0413],\n",
      "        [0.9672, 0.5938, 0.1712, 0.8531]])\n",
      "tensor([[0.8531, 0.1712, 0.5938, 0.9672],\n",
      "        [0.0413, 0.0525, 0.3921, 0.8607],\n",
      "        [0.9355, 0.1387, 0.9829, 0.1129]])\n",
      "tensor([[0.8531, 0.1712, 0.5938, 0.9672],\n",
      "        [0.0413, 0.0525, 0.3921, 0.8607],\n",
      "        [0.9355, 0.1387, 0.9829, 0.1129]])\n",
      "torch.Size([6, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor变形操作\n",
    "a = torch.arange(16)\n",
    "\n",
    "print(torch.reshape(a, (4, -1)))\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "b = a.view(4, 4)\n",
    "print(b.T)\n",
    "print(torch.t(b))\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "a = torch.rand(3, 4)\n",
    "# 交换指定维度\n",
    "print(a.shape)\n",
    "print(torch.transpose(a, 0, 1).shape)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "c = a.view(3, 1, 4)\n",
    "# 删除维度值为1的维度\n",
    "out = torch.squeeze(c)\n",
    "print(c.shape)\n",
    "print(out.shape)\n",
    "# 添加纬度值为1的维度\n",
    "out = torch.unsqueeze(out, -1)\n",
    "print(out.shape)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print(a)\n",
    "# 按照dim拆分tensor，返回一个元组\n",
    "out = torch.unbind(a, dim=1)\n",
    "print(out)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "# 按照指定维度进行翻转\n",
    "print(a)\n",
    "print(torch.flip(a, dims=[1, 0]))\n",
    "print(torch.rot90(a, 2)) # 逆时针旋转a90度2次，第2个参数大于0逆时针，小于0顺时针\n",
    "a = torch.rand(3, 5, 6)\n",
    "# 也可以指定哪2个维度旋转\n",
    "print(torch.rot90(a, 1, dims=[0, 2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "be2a1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1400, 3.1400, 3.1400],\n",
       "        [3.1400, 3.1400, 3.1400]])"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor填充操作\n",
    "torch.full((2, 3), 3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "77e3239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 频谱操作: torch.fft torch.stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "449cf321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# tensor <-> numpy\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "k = a.numpy()\n",
    "print(k)\n",
    "print(torch.from_numpy(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe70895",
   "metadata": {},
   "source": [
    "# 深度学习相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259ec31",
   "metadata": {},
   "source": [
    "模型保存与加载  \n",
    "torch.saves(state, dir) 保存/序列化  \n",
    "torch.load(dir) 加载模型\n",
    "\n",
    "并行化  \n",
    "torch.get_num_threads()  \n",
    "torch.set_num_threads(int) 设置用于并行化cpu操作的OpenMP线程数\n",
    "\n",
    "GPU  \n",
    "a = torch.tensor([1, 2, 3])  \n",
    "a.to('cuda')\n",
    "\n",
    "Variable已经合并到Tensor(小写tensor)\n",
    "\n",
    "梯度计算  \n",
    "torch.autograd.backward|torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "a7886f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[36.,  0.],\n",
      "        [ 0., 36.]])\n",
      "<AddBackward0 object at 0x000001BA29BC4280>\n",
      "<MulBackward0 object at 0x000001BA29BC4550>\n",
      "tensor([[72.,  0.],\n",
      "        [ 0., 72.]])\n",
      "<AddBackward0 object at 0x000001BA29BC47F0>\n",
      "<MulBackward0 object at 0x000001BA29BC4550>\n",
      "(tensor([[18.,  0.],\n",
      "        [ 0., 18.]]), tensor([[18.,  0.],\n",
      "        [ 0., 18.]]), tensor([[1., 0.],\n",
      "        [0., 1.]]))\n"
     ]
    }
   ],
   "source": [
    "# 梯度计算\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "# 钩子函数，对计算得到的梯度做其他的处理\n",
    "x.register_hook(lambda grad: grad * 2)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "\n",
    "# 参数是权重矩阵，最终输出结果会与tensors对应的梯度矩阵相乘\n",
    "w = torch.Tensor([[1, 0], [0, 1]])\n",
    "z.backward(w, retain_graph=True)\n",
    "print(x.grad) # y,z=None\n",
    "print(y.grad_fn) # x=None\n",
    "print(z.grad_fn)\n",
    "\n",
    "# 梯度默认会被累加\n",
    "torch.autograd.backward(z, grad_tensors=w, retain_graph=True)\n",
    "print(x.grad) # y,z=None\n",
    "print(y.grad_fn) # x=None\n",
    "print(z.grad_fn)\n",
    "\n",
    "# 这个可以直接打印的\n",
    "# 1.钩子函数不会生效\n",
    "# 2.梯度不会被累加\n",
    "print(torch.autograd.grad(z, [x, y, z], grad_outputs=w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "ab779fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.7603, 0.3122],\n",
      "        [0.1599, 0.2643]], requires_grad=True) tensor([[0.9128, 0.6314],\n",
      "        [0.2291, 0.8903]], requires_grad=True) tensor([[0.4087, 0.6699],\n",
      "        [0.2634, 0.4022]], requires_grad=True)\n",
      "tensor([[0.9128, 0.6314],\n",
      "        [0.2291, 0.8903]]) tensor([[0.7603, 0.3122],\n",
      "        [0.1599, 0.2643]]) tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.autograd.Function\n",
    "每个原始自动求导运算是2个在Tensor上运行的函数=forward（前向传播）+backward（反向梯度求导）\n",
    "\"\"\"\n",
    "class line(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        \"\"\"\n",
    "        ctx: 上下文管理器\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(w, x, b)\n",
    "        return w * x + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        \"\"\"\n",
    "        grad_out: 上一级的导数（链式法则）\n",
    "        \"\"\"\n",
    "        w, x, b = ctx.saved_tensors\n",
    "        grad_w = grad_out * x\n",
    "        grad_x = grad_out * w\n",
    "        grad_b = grad_out\n",
    "        return grad_w, grad_x, grad_b\n",
    "    \n",
    "w = torch.rand(2, 2, requires_grad=True)\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "b = torch.rand(2, 2, requires_grad=True)\n",
    "\n",
    "out = line.apply(w, x, b)\n",
    "out.backward(torch.ones(2, 2))\n",
    "\n",
    "print(w, x, b)\n",
    "print(w.grad, x.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69bdb9",
   "metadata": {},
   "source": [
    "torch.nn库：专门为神经网络设计的模块化接口，构建于autograd之上，可以用来定义和运行神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ac808",
   "metadata": {},
   "source": [
    "可视化工具：visdom  tensorboardX  torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca61ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5bbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24e1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
