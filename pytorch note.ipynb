{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "887f613b",
   "metadata": {},
   "source": [
    "# 基础数据操作相关API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2ad47d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5b2714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2., 3.],\n",
      "        [4., 5., 6.]])\n",
      "torch.float32\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor([[1,2,3],[4,5,6]])\n",
    "print(a)\n",
    "print(a.dtype)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03b05272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.Tensor(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4e9f6d8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "torch.FloatTensor\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2, 3)\n",
    "print(a)\n",
    "print(a.type())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f65fb82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.eye(2, 3) # 一般用于生成单位矩阵\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4679813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.zeros(2, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a7e0050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "b = torch.ones_like(a) # 与a形状相同的\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d62e7ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3119, 0.2701],\n",
      "        [0.1118, 0.1012]])\n",
      "tensor([0.1790, 0.0072, 0.0977, 0.1582, 0.0467])\n",
      "tensor([0.3437, 0.4959, 0.9157, 1.3216, 0.5133])\n",
      "[[  0.89972893  98.69870921]\n",
      " [ -0.39162636  99.02854393]\n",
      " [  0.44296476  98.57203403]\n",
      " [ -0.11905392  97.55355544]\n",
      " [ -0.98204824  98.24415647]\n",
      " [  0.5515635  101.99574823]\n",
      " [  0.64849386  98.17729473]\n",
      " [  0.86591366 101.70796977]\n",
      " [  0.63668575  99.24054837]\n",
      " [ -1.01871948  96.28926347]]\n"
     ]
    }
   ],
   "source": [
    "# 随机分布\n",
    "# 随机种子\n",
    "torch.manual_seed(666)\n",
    "a = torch.rand(2, 2) # 从[0, 1)之间的均匀分布中采样\n",
    "print(a)\n",
    "a = torch.normal(mean=0, std=torch.rand(5)) # 5个分布中每个分布随机采1个值(均值相同)\n",
    "print(a)\n",
    "a = torch.normal(mean=torch.rand(5), std=torch.rand(5)) # 5个分布中每个分布随机采1个值\n",
    "print(a)\n",
    "a = np.random.normal(loc=[0, 100], scale=[1, 2], size=[10, 2]) \n",
    "# numpy也可以！同时从2个分布中采样，“一列”对应一个分布，torch还不支持这种操作\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629cfd28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.9455,  0.8970, -0.0214, -0.6116, -0.9356],\n",
       "        [-0.9021,  0.0781,  0.1216,  0.0352,  0.5000],\n",
       "        [-0.0140, -0.0863, -0.5171, -0.2051, -0.4201],\n",
       "        [-0.3245, -0.7334, -0.0879,  0.5488, -0.2464],\n",
       "        [ 0.4038,  0.1165,  0.6974, -0.2809,  0.7632]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.Tensor(5, 5).uniform_(-1, 1) # 均匀分布\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98225024",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5238ac89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor([ 2.,  6., 10.])\n"
     ]
    }
   ],
   "source": [
    "# 序列\n",
    "a = torch.arange(1, 10, 1)\n",
    "print(a)\n",
    "a = torch.linspace(2, 10, 3)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0689356d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([9, 8, 1, 6, 0, 5, 2, 4, 3, 7])\n",
      "[1 7 5 9 0 6 2 3 4 8]\n"
     ]
    }
   ],
   "source": [
    "a = torch.randperm(10) # 打乱索引\n",
    "print(a)\n",
    "a = np.random.permutation(10)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64dd25f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4b6389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 2.])\n",
      "tensor([[0, 0, 3, 0],\n",
      "        [4, 0, 5, 0]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# tensor的属性\n",
    "# torch.device('cuda:0') mac m1没有cuda...\n",
    "a = torch.tensor([1,2], dtype=torch.float32, device=torch.device('cpu'))\n",
    "print(a)\n",
    "# 稀疏张量\n",
    "indices = torch.tensor([[0, 1, 1], [2, 0, 2]])\n",
    "values = torch.tensor([3, 4, 5], dtype=torch.float32)\n",
    "x = torch.sparse_coo_tensor(indices, values, [2, 4], dtype=torch.int32, device=torch.device('cpu'))\n",
    "print(x.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0eab8a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([6, 8])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([24, 40])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "tensor([[10., 10.],\n",
      "        [40., 40.]])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "torch.Size([1, 2, 3, 2])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[ 9., 16.]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[8.1031e+03, 8.8861e+06]])\n",
      "tensor([[  90.0171, 2980.9580]])\n",
      "tensor([[  90.0171, 2980.9580]])\n",
      "tensor([0.0000, 1.0000, 0.6021])\n",
      "tensor([0.0000, 1.0000, 0.6021])\n",
      "tensor([0.0000, 2.3026, 1.3863])\n"
     ]
    }
   ],
   "source": [
    "# 代数运算\n",
    "# 加减: add, sub\n",
    "a = torch.tensor([2,3])\n",
    "b = torch.tensor([4,5])\n",
    "c = a + b\n",
    "print(c)\n",
    "c = torch.add(a, b)\n",
    "print(c)\n",
    "c = a.add(b)\n",
    "print(c)\n",
    "a.add_(b) # a = a + b 覆盖a的值，其他运算下划线含义一致\n",
    "print(a)\n",
    "\n",
    "# 乘 mul，为哈达玛积（element wise，对应元素相乘），除法div含义类似\n",
    "c = a * b\n",
    "print(c)\n",
    "c = torch.mul(a, b)\n",
    "print(c)\n",
    "c = a.mul(b)\n",
    "print(c)\n",
    "a.mul_(b)\n",
    "print(a)\n",
    "\n",
    "# 矩阵运算\n",
    "a = torch.Tensor([[1], [4]])\n",
    "b = torch.ones(1, 2) * 10\n",
    "print(torch.matmul(a, b))\n",
    "print(a @ b)\n",
    "print(a.matmul(b))\n",
    "\n",
    "# 高维度的矩阵运算同样支持，但矩阵乘法仅定义在最后2个维度上，且要求前面的维度保持一致\n",
    "a = torch.ones(1, 2, 3, 4)\n",
    "b = torch.ones(1, 2, 4, 2)\n",
    "print((a @ b).shape)\n",
    "print(a.matmul(b).shape)\n",
    "print(torch.matmul(a, b).shape)\n",
    "\n",
    "# 幂运算\n",
    "a = torch.Tensor([[3, 4]])\n",
    "print(torch.pow(a, 2))\n",
    "print(a.pow(2))\n",
    "print(a**2)\n",
    "a.pow_(2)\n",
    "print(a)\n",
    "print(torch.exp(a)) # e^a\n",
    "print(a.exp())\n",
    "a.exp_()\n",
    "print(a)\n",
    "\n",
    "# 开根号\n",
    "print(a.sqrt())\n",
    "a.sqrt_()\n",
    "print(a)\n",
    "\n",
    "# 对数运算\n",
    "a = torch.tensor([1, 10, 4], dtype=torch.float32)\n",
    "print(a.log10())\n",
    "print(torch.log10(a))\n",
    "a.log_() # log默认以e为底\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6d1a481",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9742, 0.5542, 0.3176]],\n",
      "\n",
      "        [[1.0227, 0.6386, 0.4340]]])\n",
      "torch.Size([2, 1, 3])\n",
      "[[[[1.13525743 0.91870158 1.4076906 ]\n",
      "   [0.95757732 1.08441379 0.81048155]]\n",
      "\n",
      "  [[1.67793937 0.80918156 1.72866985]\n",
      "   [0.79727803 1.30251086 0.76302357]]\n",
      "\n",
      "  [[1.09840528 1.83940709 1.22648903]\n",
      "   [1.48694203 1.04813977 1.13788794]]\n",
      "\n",
      "  [[1.49341237 0.61484914 0.67252211]\n",
      "   [1.58877792 1.09710088 0.4642748 ]]]\n",
      "\n",
      "\n",
      " [[[0.94791627 0.06657935 0.89284814]\n",
      "   [0.77023616 0.23229155 0.29563909]]\n",
      "\n",
      "  [[1.55296856 0.31881933 1.59687526]\n",
      "   [0.67230722 0.81214862 0.63122899]]\n",
      "\n",
      "  [[1.03456643 1.67195573 1.33107982]\n",
      "   [1.42310319 0.88068842 1.24247873]]\n",
      "\n",
      "  [[0.91126136 1.36020366 1.16159037]\n",
      "   [1.00662691 1.84245541 0.95334306]]]]\n",
      "(2, 4, 2, 3)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "in-place操作：就地操作，也称为原位操作，不允许使用临时变量，如加法add_等带下划线的操作\n",
    "广播机制：满足一定条件，参与运算的2个变量的shape可自动对齐\n",
    "1.每个张量至少有一个维度\n",
    "2.满足右对齐：如a->shape(2, 1, 3), b->shape(3,),a+b时,b会被扩展到(1, 1, 3),在逐个比较shape的每个维度满足：值相同或有一个为1\n",
    "\"\"\"\n",
    "c = torch.rand(2, 1, 3) + torch.rand(3,)\n",
    "print(c)\n",
    "print(c.shape)\n",
    "\n",
    "# numpy同样满足广播机制\n",
    "c = np.random.rand(2, 4, 1, 3) + np.random.rand(4, 2, 3)\n",
    "print(c)\n",
    "print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f3b698c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.1581, 8.6200],\n",
      "        [3.4052, 7.0991]])\n",
      "tensor([[6., 8.],\n",
      "        [3., 7.]])\n",
      "tensor([[7., 9.],\n",
      "        [4., 8.]])\n",
      "tensor([[6., 9.],\n",
      "        [3., 7.]])\n",
      "tensor([[6., 8.],\n",
      "        [3., 7.]])\n",
      "tensor([[0.1581, 0.6200],\n",
      "        [0.4052, 0.0991]])\n",
      "tensor([[0.1581, 0.6200],\n",
      "        [1.4052, 1.0991]])\n"
     ]
    }
   ],
   "source": [
    "# 取整｜取余\n",
    "a = torch.rand(2, 2)\n",
    "a *= 10\n",
    "print(a)\n",
    "print(torch.floor(a)) # 向上取整\n",
    "print(torch.ceil(a)) # 向下取整\n",
    "print(torch.round(a)) # 四舍五入\n",
    "print(torch.trunc(a)) # 裁剪，只取整数部分\n",
    "print(torch.frac(a)) # 只取小数部分\n",
    "print(a % 2) # 取余"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4564e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[ True,  True,  True],\n",
      "        [ True, False,  True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False,  True, False]])\n",
      "tensor([[False, False, False],\n",
      "        [False,  True, False]])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# tensor比较运算\n",
    "a = torch.tensor([[2, 1, 3], [5, 1, 2]])\n",
    "b = torch.tensor([[2, 1, 3], [5, 2, 2]])\n",
    "print(a == b)\n",
    "print(torch.eq(a, b)) # 逐个比较a和b中的元素，相等返回true，否则返回false，返回值也是一个tensor\n",
    "print(torch.ge(a, b)) # a >= b\n",
    "print(torch.gt(a, b)) # a > b\n",
    "print(torch.le(a, b)) # a <= b\n",
    "print(torch.lt(a, b)) # a < b\n",
    "print(torch.ne(a, b)) # a != b\n",
    "\n",
    "a = torch.tensor([[2, 1, 3], [5, 1, 2]])\n",
    "b = torch.tensor([[2, 1, 3], [5, 1, 2], [4, 1, 4]])\n",
    "print(torch.equal(a, b)) # 返回值为true或false，shape和每个对应元素都相同才返回true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "81f0449c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0871, 0.2084, 0.2247, 0.5649, 0.9904],\n",
      "        [0.0094, 0.1050, 0.2267, 0.6248, 0.6446],\n",
      "        [0.0290, 0.0578, 0.1222, 0.7299, 0.8924]])\n",
      "tensor([[0, 3, 4, 1, 2],\n",
      "        [3, 4, 0, 1, 2],\n",
      "        [2, 0, 3, 4, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 排序｜前k大｜前k小｜第k小\n",
    "a = torch.rand(3, 5)\n",
    "# sort的第二个参数dim，0是沿着第一个维度（每列有序），1沿着第二个维度（每行有序），\n",
    "# -1等同于倒数第一个维度，-2依次类推，默认值-1\n",
    "sorts, indices = torch.sort(a)\n",
    "print(sorts)\n",
    "print(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2993b9f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([[5, 4],\n",
       "        [6, 5]]),\n",
       "indices=tensor([[4, 1],\n",
       "        [1, 2]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[2, 4, 3, 1, 5], [2, 6, 5, 1, 4]])\n",
    "# 寻找前k大的元素以及对应的索引\n",
    "torch.topk(a, 2, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "de97f044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.kthvalue(\n",
       "values=tensor([3, 4]),\n",
       "indices=tensor([2, 4]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 寻找按某个维度第k小的元素\n",
    "torch.kthvalue(a, 3, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "733eb209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5562, 0.1525, 0.7820],\n",
      "        [0.5986, 0.7850, 0.5672]])\n",
      "tensor([[inf, inf, inf],\n",
      "        [inf, inf, inf]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([[True, True, True],\n",
      "        [True, True, True]])\n",
      "tensor([[False, False, False],\n",
      "        [False, False, False]])\n",
      "tensor([False,  True, False])\n"
     ]
    }
   ],
   "source": [
    "# 判断nan，inf，以及是否有界\n",
    "a = torch.rand(2, 3)\n",
    "print(a)\n",
    "print(a / 0)\n",
    "print(torch.isinf(a / 0)) # 判断是否为inf\n",
    "print(torch.isfinite(a / 0)) # 判断是否有界\n",
    "print(torch.isfinite(a))\n",
    "print(torch.isnan(a)) # 判断是否为nan\n",
    "a = torch.tensor([1, np.nan, 2])\n",
    "print(torch.isnan(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "411d47c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.8415, -0.9093,  0.0000])\n",
      "tensor([ 0.5403, -0.4161,  1.0000])\n"
     ]
    }
   ],
   "source": [
    "# torch中的三角函数\n",
    "a = torch.tensor([1, -2, 0])\n",
    "print(torch.sin(a))\n",
    "print(torch.cos(a))\n",
    "# ......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74bf3eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 0])\n",
      "tensor([ 1.0000, -0.5000,     inf])\n",
      "tensor([0.7311, 0.1192, 0.5000])\n",
      "tensor([1, 0, 0])\n",
      "tensor([ 1, -1,  0])\n"
     ]
    }
   ],
   "source": [
    "# torch中其他数学函数\n",
    "print(torch.abs(a))\n",
    "print(torch.reciprocal(a)) # 求倒数\n",
    "print(torch.sigmoid(a))\n",
    "print(torch.relu(a))\n",
    "print(torch.sign(a)) # 符号函数\n",
    "# ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "010bbae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.8333)\n",
      "torch.return_types.mode(\n",
      "values=tensor([-2.,  4.]),\n",
      "indices=tensor([1, 0]))\n",
      "tensor([1., 2., 0., 1., 2.])\n",
      "tensor([6, 0, 1, 6, 9, 0, 2, 1, 9, 7])\n",
      "tensor([2, 2, 1, 0, 0, 0, 2, 1, 0, 2])\n"
     ]
    }
   ],
   "source": [
    "# torch统计学函数：torch.mean, torch.max, torch.std......\n",
    "a = torch.tensor([[1, -2, 0], [4, 6, 8]], dtype=torch.float32)\n",
    "print(torch.mean(a))\n",
    "print(torch.mode(a))\n",
    "# ......\n",
    "\n",
    "# 计算直方图：参数：（input，bin，min，max）\n",
    "# 第二个参数bin表示等距划分为多少个区间（每个区间左闭右开，最后一个区间两边都是闭合的）\n",
    "# min和max表示统计的范围，当输入的min=max时，则取输入数据的最小值最大值替代；小于min和大于max的元素将被忽略\n",
    "print(torch.histc(a, 5, 0, 0))\n",
    "\n",
    "# 返回每个值的频数\n",
    "a = torch.randint(0, 10, (10,))\n",
    "print(a)\n",
    "# bincount只支持一维非负整数（比如处理聚类|分类结果）\n",
    "print(torch.bincount(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ef530eae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.distributions 定义分布函数\n",
    "# torch中还定义了衡量分布差异性的指标，如KL散度等\n",
    "# kl divergence|transforms|constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "75b4e7c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.1962)\n",
      "tensor(1.4625)\n"
     ]
    }
   ],
   "source": [
    "# 范数\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "b = torch.Tensor([4, 5, 6])\n",
    "print(torch.dist(a, b, p=2))\n",
    "\n",
    "# 核范数（用于低秩约束）\n",
    "# torch.manual_seed(666)\n",
    "a = torch.rand(5, 2)\n",
    "print(torch.norm(a, p='nuc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "fb222d46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.svd(\n",
       "U=tensor([[-0.8513,  0.2673],\n",
       "        [-0.4227, -0.6957],\n",
       "        [-0.2981,  0.0272],\n",
       "        [-0.0616,  0.4282],\n",
       "        [-0.0628,  0.5104]]),\n",
       "S=tensor([1.1951, 0.2675]),\n",
       "V=tensor([[-0.4724,  0.8814],\n",
       "        [-0.8814, -0.4724]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 矩阵分解（LU（上三角+下三角），QR（正交矩阵+上三角），EVD（PCA），SVD（LDA））\n",
    "# 详细内容见ipad笔记\n",
    "torch.svd(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0d29d0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1190, 2.7013],\n",
      "        [1.1178, 1.0119]])\n",
      "tensor([[3.1190, 2.7013],\n",
      "        [2.0000, 2.0000]])\n"
     ]
    }
   ],
   "source": [
    "# tensor裁剪运算\n",
    "torch.manual_seed(666)\n",
    "a = torch.rand(2, 2) * 10\n",
    "print(a)\n",
    "print(a.clamp(2, 6)) # 数据保持在给定范围内，超过该范围替换为端点值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "18fbe581",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1877, 0.0181, 0.3317, 0.0846],\n",
      "        [0.5732, 0.0079, 0.2520, 0.5518],\n",
      "        [0.8785, 0.5281, 0.4961, 0.9791],\n",
      "        [0.5817, 0.4875, 0.0650, 0.7506]])\n",
      "tensor([[0.2634, 0.3684, 0.5035, 0.9089],\n",
      "        [0.3804, 0.7539, 0.4163, 0.0089],\n",
      "        [0.0664, 0.8111, 0.2667, 0.0601],\n",
      "        [0.3079, 0.2885, 0.8916, 0.6119]])\n",
      "tensor([[0.2634, 0.3684, 0.5035, 0.9089],\n",
      "        [0.5732, 0.7539, 0.4163, 0.5518],\n",
      "        [0.8785, 0.5281, 0.2667, 0.9791],\n",
      "        [0.5817, 0.2885, 0.8916, 0.7506]])\n",
      "index_select------------------------------------------------------\n",
      "tensor([[0.4041, 0.4576, 0.4031, 0.0272],\n",
      "        [0.9485, 0.4893, 0.1942, 0.0322],\n",
      "        [0.0489, 0.5390, 0.5608, 0.5176],\n",
      "        [0.7500, 0.4930, 0.4568, 0.2414]])\n",
      "tensor([[0.4041, 0.4576, 0.4031, 0.0272],\n",
      "        [0.7500, 0.4930, 0.4568, 0.2414],\n",
      "        [0.0489, 0.5390, 0.5608, 0.5176]])\n",
      "tensor([[0.4041, 0.4576, 0.4031, 0.0272],\n",
      "        [0.7500, 0.4930, 0.4568, 0.2414],\n",
      "        [0.0489, 0.5390, 0.5608, 0.5176]])\n",
      "gather------------------------------------------------------\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([[1, 6, 7, 8],\n",
      "        [9, 6, 3, 4]])\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12],\n",
      "        [13, 14, 15, 16]])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "tensor([ 9, 10, 11, 12, 13, 14, 15, 16])\n",
      "take------------------------------------------------------\n",
      "tensor([ 1, 16, 14, 11])\n",
      "nonzero------------------------------------------------------\n",
      "tensor([[0, 1],\n",
      "        [0, 2],\n",
      "        [1, 0],\n",
      "        [1, 1],\n",
      "        [1, 3]])\n"
     ]
    }
   ],
   "source": [
    "# tensor索引 数据筛选|重新组合\n",
    "a = torch.rand(4, 4)\n",
    "b = torch.rand(4, 4)\n",
    "print(a)\n",
    "print(b)\n",
    "# 每个位置根据条件判断，满足条件选择a的元素，否则选择b中的元素\n",
    "out = torch.where(a > 0.5, a, b)\n",
    "print(out)\n",
    "\n",
    "print('index_select------------------------------------------------------')\n",
    "a = torch.rand(4, 4)\n",
    "print(a)\n",
    "# 指定维度上将不同索引的数据按照指定顺序重新组合\n",
    "print(torch.index_select(a, dim=0, index=torch.tensor([0, 3, 2])))\n",
    "print(a[[0, 3, 2]])\n",
    "\n",
    "# gather\n",
    "print('gather------------------------------------------------------')\n",
    "a = torch.arange(1, 17).view(4, 4)\n",
    "print(a)\n",
    "# 输出shape和index的shape相同（含义看打印结果吧），也是对原数据进行重组\n",
    "print(torch.gather(a, dim=0, index=torch.tensor([[0, 1, 1, 1], [2, 1, 0, 0]])))\n",
    "\n",
    "# select\n",
    "print(a)\n",
    "mask = torch.gt(a, 8)\n",
    "print(torch.masked_select(a, mask))\n",
    "print(torch.masked_select(a, a > 8))\n",
    "print(a[a > 8])\n",
    "\n",
    "print('take------------------------------------------------------')\n",
    "# take: 类似index_select，只是将a看成1维向量，再取对应的索引\n",
    "print(torch.take(a, index=torch.tensor([0, 15, 13, 10])))\n",
    "\n",
    "print('nonzero------------------------------------------------------')\n",
    "# nonzero: 返回非0元素的索引\n",
    "a = torch.tensor([[0, 1, 2, 0], [2, 3, 0, 1]])\n",
    "out = torch.nonzero(a)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c88954a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([2, 3])\n",
      "torch.Size([3, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor组合和拼接: torch.cat torch.stack\n",
    "a = torch.zeros(2, 4)\n",
    "b = torch.ones(2, 4)\n",
    "out = torch.cat([a, b], dim=0)\n",
    "print(out)\n",
    "\n",
    "# stack是拓展了新的维度再堆叠数据\n",
    "a = torch.arange(1, 7).view(2, 3)\n",
    "b = torch.arange(7, 13).view(2, 3)\n",
    "c = torch.arange(7, 13).view(2, 3)\n",
    "out = torch.stack([a, b, c], dim=0)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1b6536d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[0.3975, 0.2900, 0.3378, 0.1333],\n",
      "        [0.4560, 0.7744, 0.3768, 0.7019],\n",
      "        [0.5582, 0.8487, 0.3596, 0.8816]]), tensor([[0.6317, 0.6924, 0.1204, 0.9379],\n",
      "        [0.6783, 0.6848, 0.2201, 0.4572]]))\n",
      "(tensor([[0.3975, 0.2900],\n",
      "        [0.4560, 0.7744],\n",
      "        [0.5582, 0.8487],\n",
      "        [0.6317, 0.6924],\n",
      "        [0.6783, 0.6848]]), tensor([[0.3378, 0.1333],\n",
      "        [0.3768, 0.7019],\n",
      "        [0.3596, 0.8816],\n",
      "        [0.1204, 0.9379],\n",
      "        [0.2201, 0.4572]]))\n",
      "(tensor([[0.3975, 0.2900, 0.3378, 0.1333],\n",
      "        [0.4560, 0.7744, 0.3768, 0.7019]]), tensor([[0.5582, 0.8487, 0.3596, 0.8816],\n",
      "        [0.6317, 0.6924, 0.1204, 0.9379]]), tensor([[0.6783, 0.6848, 0.2201, 0.4572]]))\n",
      "------------------------------------------------------\n",
      "(tensor([[0.3975, 0.2900, 0.3378, 0.1333]]), tensor([[0.4560, 0.7744, 0.3768, 0.7019],\n",
      "        [0.5582, 0.8487, 0.3596, 0.8816]]), tensor([[0.6317, 0.6924, 0.1204, 0.9379],\n",
      "        [0.6783, 0.6848, 0.2201, 0.4572]]))\n"
     ]
    }
   ],
   "source": [
    "# tensor切片: chunk split\n",
    "a = torch.rand(5, 4)\n",
    "# chunk：第2个参数表示切成多少份\n",
    "print(torch.chunk(a, 2, dim=0))\n",
    "print(torch.chunk(a, 2, dim=1))\n",
    "\n",
    "# split：第2个参数表示每份多少个，如输入3表示每份3个，输入[1, 2, ...]定义了每份多少个，不规则切片\n",
    "out = torch.split(a, 2, dim=0)\n",
    "print(out)\n",
    "print('------------------------------------------------------')\n",
    "out = torch.split(a, [1, 2, 2], dim=0)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d83c2ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "------------------------------------------------------\n",
      "tensor([[ 0,  4,  8, 12],\n",
      "        [ 1,  5,  9, 13],\n",
      "        [ 2,  6, 10, 14],\n",
      "        [ 3,  7, 11, 15]])\n",
      "tensor([[ 0,  4,  8, 12],\n",
      "        [ 1,  5,  9, 13],\n",
      "        [ 2,  6, 10, 14],\n",
      "        [ 3,  7, 11, 15]])\n",
      "------------------------------------------------------\n",
      "torch.Size([3, 4])\n",
      "torch.Size([4, 3])\n",
      "------------------------------------------------------\n",
      "torch.Size([3, 1, 4])\n",
      "torch.Size([3, 4])\n",
      "torch.Size([3, 4, 1])\n",
      "------------------------------------------------------\n",
      "tensor([[0.6167, 0.4009, 0.2426, 0.0523],\n",
      "        [0.4494, 0.3270, 0.1688, 0.5733],\n",
      "        [0.3116, 0.2653, 0.6158, 0.8620]])\n",
      "(tensor([0.6167, 0.4494, 0.3116]), tensor([0.4009, 0.3270, 0.2653]), tensor([0.2426, 0.1688, 0.6158]), tensor([0.0523, 0.5733, 0.8620]))\n",
      "------------------------------------------------------\n",
      "tensor([[0.6167, 0.4009, 0.2426, 0.0523],\n",
      "        [0.4494, 0.3270, 0.1688, 0.5733],\n",
      "        [0.3116, 0.2653, 0.6158, 0.8620]])\n",
      "tensor([[0.8620, 0.6158, 0.2653, 0.3116],\n",
      "        [0.5733, 0.1688, 0.3270, 0.4494],\n",
      "        [0.0523, 0.2426, 0.4009, 0.6167]])\n",
      "tensor([[0.8620, 0.6158, 0.2653, 0.3116],\n",
      "        [0.5733, 0.1688, 0.3270, 0.4494],\n",
      "        [0.0523, 0.2426, 0.4009, 0.6167]])\n",
      "torch.Size([6, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "# tensor变形操作\n",
    "a = torch.arange(16)\n",
    "\n",
    "print(torch.reshape(a, (4, -1)))\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "b = a.view(4, 4)\n",
    "print(b.T)\n",
    "print(torch.t(b))\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "a = torch.rand(3, 4)\n",
    "# 交换指定维度\n",
    "print(a.shape)\n",
    "print(torch.transpose(a, 0, 1).shape)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "c = a.view(3, 1, 4)\n",
    "# 删除维度值为1的维度\n",
    "out = torch.squeeze(c)\n",
    "print(c.shape)\n",
    "print(out.shape)\n",
    "# 添加纬度值为1的维度\n",
    "out = torch.unsqueeze(out, -1)\n",
    "print(out.shape)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "print(a)\n",
    "# 按照dim拆分tensor，返回一个元组\n",
    "out = torch.unbind(a, dim=1)\n",
    "print(out)\n",
    "\n",
    "print('------------------------------------------------------')\n",
    "# 按照指定维度进行翻转\n",
    "print(a)\n",
    "print(torch.flip(a, dims=[1, 0]))\n",
    "print(torch.rot90(a, 2)) # 逆时针旋转a90度2次，第2个参数大于0逆时针，小于0顺时针\n",
    "a = torch.rand(3, 5, 6)\n",
    "# 也可以指定哪2个维度旋转\n",
    "print(torch.rot90(a, 1, dims=[0, 2]).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "be2a1cb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.1400, 3.1400, 3.1400],\n",
       "        [3.1400, 3.1400, 3.1400]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tensor填充操作\n",
    "torch.full((2, 3), 3.14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "77e3239d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 频谱操作: torch.fft torch.stft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "449cf321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 2. 3.]\n",
      "tensor([1., 2., 3.])\n"
     ]
    }
   ],
   "source": [
    "# tensor <-> numpy\n",
    "a = torch.Tensor([1, 2, 3])\n",
    "k = a.numpy()\n",
    "print(k)\n",
    "print(torch.from_numpy(k))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe70895",
   "metadata": {},
   "source": [
    "# 深度学习相关"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5259ec31",
   "metadata": {},
   "source": [
    "模型保存与加载  \n",
    "torch.saves(state, dir) 保存/序列化  \n",
    "torch.load(dir) 加载模型\n",
    "\n",
    "并行化  \n",
    "torch.get_num_threads()  \n",
    "torch.set_num_threads(int) 设置用于并行化cpu操作的OpenMP线程数\n",
    "\n",
    "GPU  \n",
    "a = torch.tensor([1, 2, 3])  \n",
    "a.to('cuda')\n",
    "\n",
    "Variable已经合并到Tensor(小写tensor)\n",
    "\n",
    "梯度计算  \n",
    "torch.autograd.backward|torch.autograd.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a7886f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>)\n",
      "tensor([[36.,  0.],\n",
      "        [ 0., 36.]])\n",
      "<AddBackward0 object at 0x0000029D8D31ECD0>\n",
      "<MulBackward0 object at 0x0000029D8D31E9D0>\n",
      "tensor([[72.,  0.],\n",
      "        [ 0., 72.]])\n",
      "<AddBackward0 object at 0x0000029D8D31ECD0>\n",
      "<MulBackward0 object at 0x0000029D8D31E9D0>\n",
      "(tensor([[18.,  0.],\n",
      "        [ 0., 18.]]), tensor([[18.,  0.],\n",
      "        [ 0., 18.]]), tensor([[1., 0.],\n",
      "        [0., 1.]]))\n",
      "tensor([[72.,  0.],\n",
      "        [ 0., 72.]])\n",
      "<AddBackward0 object at 0x0000029D8D31ECD0>\n",
      "<MulBackward0 object at 0x0000029D8D31E9D0>\n"
     ]
    }
   ],
   "source": [
    "# 梯度计算\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "# 钩子函数，对计算得到的梯度做其他的处理\n",
    "x.register_hook(lambda grad: grad * 2)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "print(z)\n",
    "\n",
    "# 参数是权重矩阵，最终输出结果会与tensors对应的梯度矩阵相乘\n",
    "w = torch.Tensor([[1, 0], [0, 1]])\n",
    "z.backward(w, retain_graph=True)\n",
    "print(x.grad) # y,z=None\n",
    "print(y.grad_fn) # x=None\n",
    "print(z.grad_fn)\n",
    "\n",
    "# 梯度默认会被累加\n",
    "torch.autograd.backward(z, grad_tensors=w, retain_graph=True)\n",
    "print(x.grad) # y,z=None\n",
    "print(y.grad_fn) # x=None\n",
    "print(z.grad_fn)\n",
    "\n",
    "# 这个可以直接打印的\n",
    "# 1.钩子函数不会生效\n",
    "# 2.梯度不会被累加\n",
    "print(torch.autograd.grad(z, [x, y, z], grad_outputs=w))\n",
    "print(x.grad) # y,z=None\n",
    "print(y.grad_fn) # x=None\n",
    "print(z.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ab779fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0791, 0.5924],\n",
      "        [0.0009, 0.3566]], requires_grad=True) tensor([[0.8753, 0.8435],\n",
      "        [0.9500, 0.5396]], requires_grad=True) tensor([[0.3379, 0.6051],\n",
      "        [0.5269, 0.6005]], requires_grad=True)\n",
      "tensor([[0.8753, 0.8435],\n",
      "        [0.9500, 0.5396]]) tensor([[0.0791, 0.5924],\n",
      "        [0.0009, 0.3566]]) tensor([[1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "torch.autograd.Function\n",
    "每个原始自动求导运算是2个在Tensor上运行的函数=forward（前向传播）+backward（反向梯度求导）\n",
    "\"\"\"\n",
    "class line(torch.autograd.Function):\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, w, x, b):\n",
    "        \"\"\"\n",
    "        ctx: 上下文管理器\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(w, x, b)\n",
    "        return w * x + b\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_out):\n",
    "        \"\"\"\n",
    "        grad_out: 上一级的导数（链式法则）\n",
    "        \"\"\"\n",
    "        w, x, b = ctx.saved_tensors\n",
    "        grad_w = grad_out * x\n",
    "        grad_x = grad_out * w\n",
    "        grad_b = grad_out\n",
    "        return grad_w, grad_x, grad_b\n",
    "    \n",
    "w = torch.rand(2, 2, requires_grad=True)\n",
    "x = torch.rand(2, 2, requires_grad=True)\n",
    "b = torch.rand(2, 2, requires_grad=True)\n",
    "\n",
    "out = line.apply(w, x, b)\n",
    "out.backward(torch.ones(2, 2))\n",
    "\n",
    "print(w, x, b)\n",
    "print(w.grad, x.grad, b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69bdb9",
   "metadata": {},
   "source": [
    "torch.nn库：专门为神经网络设计的模块化接口，构建于autograd之上，可以用来定义和运行神经网络"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7ac808",
   "metadata": {},
   "source": [
    "可视化工具：visdom  tensorboardX  torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaca61ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2daace7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cbeac76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa5bbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f24e1ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
